{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python2\n",
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "Text processing of data\n",
    "\"\"\"\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "import pickle\n",
    "import nltk\n",
    "\n",
    "from fnc_baseline.utils.score import report_score, LABELS, score_submission\n",
    "from fnc_baseline.utils.dataset import DataSet\n",
    "\n",
    "import codecs\n",
    "import sys\n",
    "#reload(sys) # for text processing\n",
    "#sys.setdefaultencoding('utf8') # for text processing\n",
    "\n",
    "# ======== Load data =======\n",
    "base_path = '/Users/Monu/NLP/Stance/code'\n",
    "def read_data(): \n",
    "    \n",
    "    # Extracting data\n",
    "    dataset = DataSet(path = base_path + '/data')\n",
    "    stances = dataset.stances\n",
    "    articles = dataset.articles\n",
    "    \n",
    "    # Data to lists\n",
    "    h, b, y = [],[],[]\n",
    "    for stance in stances:\n",
    "        y.append(LABELS.index(stance['Stance']))\n",
    "        h.append(stance['Headline'])\n",
    "        b.append(dataset.articles[stance['Body ID']])\n",
    "    y = np.asarray(y, dtype = np.int64)\n",
    "    #print(h)\n",
    "    #print(b)\n",
    "    #print(y)\n",
    "    return h, b, y\n",
    "#read_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "######################################################################################\n",
    "#Another way for loading embedding\n",
    "\n",
    "def load_embedding_from_disks(glove_filename, with_indexes=True):\n",
    "    \"\"\"\n",
    "    Read a GloVe txt file. If `with_indexes=True`, we return a tuple of two dictionnaries\n",
    "    `(word_to_index_dict, index_to_embedding_array)`, otherwise we return only a direct \n",
    "    `word_to_embedding_dict` dictionnary mapping from a string to a numpy array.\n",
    "    \"\"\"\n",
    "    if with_indexes:\n",
    "        word_to_index_dict = dict()\n",
    "        index_to_embedding_array = []\n",
    "    else:\n",
    "        word_to_embedding_dict = dict()\n",
    "\n",
    "    \n",
    "    with open(glove_filename, 'r') as glove_file:\n",
    "        for (i, line) in enumerate(glove_file):\n",
    "            \n",
    "            split = line.split(' ')\n",
    "            \n",
    "            word = split[0]\n",
    "            \n",
    "            representation = split[1:]\n",
    "            representation = np.array(\n",
    "                [float(val) for val in representation]\n",
    "            )\n",
    "            \n",
    "            if with_indexes:\n",
    "                word_to_index_dict[word] = i\n",
    "                index_to_embedding_array.append(representation)\n",
    "            else:\n",
    "                word_to_embedding_dict[word] = representation\n",
    "\n",
    "    _WORD_NOT_FOUND = [0.0]* len(representation)  # Empty representation for unknown words.\n",
    "    if with_indexes:\n",
    "        _LAST_INDEX = i + 1\n",
    "        word_to_index_dict = defaultdict(lambda: _LAST_INDEX, word_to_index_dict)\n",
    "        index_to_embedding_array = np.array(index_to_embedding_array + [_WORD_NOT_FOUND])\n",
    "        return word_to_index_dict, index_to_embedding_array\n",
    "    else:\n",
    "        word_to_embedding_dict = defaultdict(lambda: _WORD_NOT_FOUND)\n",
    "        return word_to_embedding_dict\n",
    "    \n",
    "'''def loadGloVe(filename):\n",
    "    vocab = []\n",
    "    embd = []\n",
    "    file = open(filename,'r')\n",
    "    for line in file.readlines():\n",
    "        row = line.strip().split(' ')\n",
    "        vocab.append(row[0])\n",
    "        embd.append(row[1:])\n",
    "    print('Loaded GloVe!')\n",
    "    file.close()\n",
    "    return vocab,embd'''\n",
    "#########################################################################################\n",
    "        \n",
    "    \n",
    "# ----- Loading Glove embeddings ----\n",
    "def loadGloVe(filename):\n",
    "    #print(filename)\n",
    "    # Getting embedding dimension\n",
    "    file0 = open(filename,'r')\n",
    "    #file0 = codecs.open(filename, 'r', 'utf8', 'ignore')\n",
    "    line = file0.readline()\n",
    "    emb_dim = len(line.strip().split(' ')) - 1\n",
    "    file0.close()\n",
    "\n",
    "    # First row of embedding matrix is 0 for zero padding\n",
    "    vocab = ['<pad>'] #By Manisha - Using this\n",
    "    embd = [[0.0] * emb_dim] #By Manisha - Using this\n",
    "    #vocab = []\n",
    "    #embd = []\n",
    "    #model = {}\n",
    "    # Reading embedding matrix\n",
    "    file = open(filename,'r')\n",
    "    file = codecs.open(filename, 'r', 'utf8', 'ignore')\n",
    "    for line in file.readlines():\n",
    "        row = line.strip().split(' ')\n",
    "        vocab.append(row[0])\n",
    "        embd.append(row[1:])\n",
    "        #model[vocab] = embd\n",
    "        #embd.append(map(float,row[1:]))\n",
    "    print('Loaded GloVe!')\n",
    "    file.close()\n",
    "    \n",
    "    return vocab,embd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# ------ Clean quote signs ---------\n",
    "def clean_data(sentences):\n",
    "    '''\n",
    "    Delete quote signs\n",
    "        - Rational: quote signs mix with the parsing\n",
    "        - Con: quote signs are meaningul --> distanciation from a statement\n",
    "    '''\n",
    "    new_sentences = []\n",
    "    for sentence in sentences:\n",
    "        new_sentences.append(sentence.replace(\"'\",\"\").replace('\"',''))\n",
    "    return new_sentences\n",
    "\n",
    "# ---- Build vocab dictionary from embedding matrix -----\n",
    "def build_vocDict(vocab):\n",
    "    voc_dict = {}\n",
    "    for i in range(len(vocab)):\n",
    "        #print(vocab[i])\n",
    "        voc_dict[vocab[i]] = i\n",
    "    return voc_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# -------- words to ids only -------\n",
    "\n",
    "def words2ids(sentences, voc_dict, option = 'simple'):\n",
    "    '''\n",
    "    Inputs: \n",
    "        - sentences: list of sentences as string\n",
    "        - embedding_vocab: list of vocab words in the order of the rows of embedding_matrix\n",
    "    Ouptut: \n",
    "        - new_sentences_ids: list of sentences as successive word indexes\n",
    "    Processing: delete word which do no appear in vocabulary\n",
    "        - Alternative: replace missing words by the mean\n",
    "    '''\n",
    "    new_sentences_ids = []\n",
    "    j = 0\n",
    "    for sentence in sentences:\n",
    "        j+=1\n",
    "        if j % 5000 == 0:\n",
    "            print ('sentence',str(j))\n",
    "        sentence_ids = []\n",
    "        if option == 'nltk':\n",
    "            sentence = sentence.decode('utf8', 'ignore')\n",
    "            # print('sentence', sentence)\n",
    "            word_list = tokenize(sentence)\n",
    "            print('word_list', word_list)\n",
    "        elif option == 'simple':\n",
    "            word_list = sentence.split(\" \")\n",
    "        \n",
    "        for word in word_list:\n",
    "            if word.lower() in voc_dict: # Only add word if in dictionary\n",
    "                word_index = voc_dict[word.lower()]\n",
    "                sentence_ids.append(word_index)\n",
    "                \n",
    "        new_sentences_ids.append(sentence_ids)\n",
    "        #print (\"added\",j)\n",
    "    return new_sentences_ids\n",
    "\n",
    "\n",
    "# -------- words to ids and vectors -------\n",
    "def words2ids_vects(sentences, voc_dict, embedding_matrix, option = 'simple'):   ###Check this.. :( - Manisha)\n",
    "    '''\n",
    "    Inputs: \n",
    "        - sentences: list of sentences as string\n",
    "        - embedding_vocab: list of vocab words in the order of the rows of embedding_matrix\n",
    "        - embedding_matrix\n",
    "    Ouptut: \n",
    "        - new_sentences_ids: list of sentences as successive word indexes\n",
    "        - new_sentences_vects: list of sentences as successive word vectors\n",
    "    Processing: delete word which do no appear in vocabulary\n",
    "        - Alternative: replace missing words by the mean\n",
    "    '''\n",
    "    #print(voc_dict)\n",
    "    new_sentences_ids = []\n",
    "    new_sentences_vects = []\n",
    "    j = 0\n",
    "    newsentences = clean_data(sentences)\n",
    "    for sentence in newsentences:\n",
    "        j+=1\n",
    "        if j % 5000 == 0:\n",
    "            print ('sentence',str(j))\n",
    "        sentence_ids = []\n",
    "        sentence_vects = []\n",
    "        if option == 'nltk':\n",
    "            #sentence = sentence.decode('utf8', 'ignore')\n",
    "            # print('sentence', sentence)\n",
    "            word_list = tokenize(sentence)\n",
    "            #print('word_list', word_list)\n",
    "        elif option == 'simple':\n",
    "            word_list = sentence.split(\" \")\n",
    "            #print('word_list', word_list)\n",
    "        for word in word_list:\n",
    "            word = word.decode(\"utf-8\")\n",
    "            #print(word)\n",
    "            #print(voc_dict[word])\n",
    "            #print(voc_dict[word.lower()])\n",
    "            if word.lower() in voc_dict: # Only add word if in dictionary                \n",
    "                word_index = voc_dict[word.lower()]\n",
    "                #print(word_index)\n",
    "                #print(embedding_matrix[word_index])\n",
    "                sentence_ids.append(word_index)\n",
    "                sentence_vects.append(embedding_matrix[word_index])\n",
    "                \n",
    "        new_sentences_ids.append(sentence_ids)\n",
    "        #print (\"added\", j)\n",
    "        #print(sentence_vects)\n",
    "        new_sentences_vects.append(sentence_vects)\n",
    "    print(new_sentences_vects)\n",
    "    return new_sentences_ids, new_sentences_vects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(sequence):\n",
    "    tokenizer = nltk.data.load('tokenizers/punkt/english.pickle')\n",
    "    tokens = [token.replace(\"``\", '\"').replace(\"''\", '\"') for token in nltk.word_tokenize(sequence)]\n",
    "    #print(tokens)\n",
    "    # return tokens\n",
    "    return map(lambda x:x.encode('utf-8', errors = 'ignore'), tokens)\n",
    "\n",
    "# ---------- Averaging vectors for headline and truncated body ---------\n",
    "\n",
    "\n",
    "def concatConvert_np(h_list, b_list):\n",
    "    '''\n",
    "    1. Concatenate headlines and bodies\n",
    "    2. Convert list data to numpy zero padded data\n",
    "    3. Also outputs sequences lengths as np vector\n",
    "    '''\n",
    "    \n",
    "    # Concatenate\n",
    "    n_sentences = len(h_list)\n",
    "    h_b_list = []\n",
    "    seqlen = []\n",
    "    for i in range(n_sentences):\n",
    "        h_b_list.append(h_list[i] + b_list[i])\n",
    "        seqlen.append(len(h_b_list[i]))\n",
    "        \n",
    "    max_len = max(seqlen)\n",
    "    \n",
    "    # Convert to numpy with zero padding. No truncating\n",
    "    h_b_np = np.zeros((n_sentences, max_len))\n",
    "    for i in range(n_sentences):\n",
    "        h_b_np[i,:seqlen[i]] = h_b_list[i]\n",
    "    \n",
    "    return h_b_list, h_b_np, np.array(seqlen)\n",
    "\n",
    "def distinctConvert_np(h_list, b_list):\n",
    "    '''\n",
    "    1. Convert list data to numpy zero padded data, 2 distinct matrices for headlines and bodies \n",
    "    2. Also outputs sequences lengths as np vector\n",
    "    '''\n",
    "    # Compute sequences lengths\n",
    "    n_sentences = len(h_list)\n",
    "    h_seqlen = []\n",
    "    b_seqlen = []\n",
    "    for i in range(n_sentences):\n",
    "        h_seqlen.append(len(h_list[i]))\n",
    "        b_seqlen.append(len(b_list[i]))\n",
    "        \n",
    "    h_max_len = max(h_seqlen)\n",
    "    b_max_len = max(b_seqlen)\n",
    "    \n",
    "    # Convert to numpy\n",
    "    h_np = np.zeros((n_sentences, h_max_len))\n",
    "    b_np = np.zeros((n_sentences, b_max_len))\n",
    "    for i in range(n_sentences):\n",
    "        h_np[i,:h_seqlen[i]] = h_list[i]\n",
    "        b_np[i,:b_seqlen[i]] = b_list[i]\n",
    "        \n",
    "    return h_np, np.array(h_seqlen), b_np, np.array(b_seqlen)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "## Updated BY Manisha\n",
    "def save_data_pickle(outfilename, \n",
    "                    embedding_type = 'twitter.27B.50d',\n",
    "                    parserOption = 'nltk'):\n",
    "    cwd = os.getcwd()\n",
    "    if embedding_type == 'twitter.27B.50d':\n",
    "        #filename_embeddings = cwd + '/../../glove/glove.twitter.27B.50d.txt'\n",
    "        filename_embeddings = base_path + '/glove/glove.twitter.27B.50d.txt'\n",
    "    else: \n",
    "        #filename_embeddings = cwd + '/../../glove/glove.6B.50d.txt'\n",
    "        filename_embeddings = base_path + '/glove/glove.6B.50d.txt'\n",
    "\n",
    "    # filename_embeddings = cwd + filename_embeddings\n",
    "\n",
    "    # GloVe embeddings\n",
    "    vocab, embd = loadGloVe(filename_embeddings)\n",
    "    vocab_size = len(vocab)\n",
    "    embedding_dim = len(embd[0])\n",
    "    embedding = np.asarray(embd)\n",
    "    #embedding = np.asarray(embd, dtype = object)\n",
    "\n",
    "    # Get vocab dict\n",
    "    voc_dict = build_vocDict(vocab)\n",
    "    #print(voc_dict['luungan'])\n",
    "    # Read and process data\n",
    "    h, b, y = read_data() #read_data(cwd + '/../../') # headline / bodies/ labels\n",
    "    \n",
    "    h_ids, h_vects = words2ids_vects(h, voc_dict, embd, parserOption)\n",
    "    #Manishacomment \n",
    "    '''b_ids, b_vects = words2ids_vects(b, voc_dict, embd, parserOption)\n",
    "    #print(h_vects)\n",
    "    # Concatenated headline_bodies zero padded np matrices; seq. lengths as np vector\n",
    "    h_b_ids, h_b_np, seqlen = concatConvert_np(h_ids, b_ids)\n",
    "    h_np, h_seqlen, b_np, b_seqlen = distinctConvert_np(h_ids, b_ids)\n",
    "\n",
    "    data_dict = {'h_ids':h_ids, 'b_ids':b_ids, 'y':y}\n",
    "    with open(outfilename, 'wb') as fp:\n",
    "        pickle.dump(data_dict, fp)'''\n",
    "    return vocab, embd\n",
    "\n",
    "## Updated BY Manisha\n",
    "def get_data(config, \n",
    "            filename_embeddings = '/glove/glove.twitter.27B.50d.txt',\n",
    "            pickle_path = '/glove/twitter50d_h_ids_b_ids_pickle.p',\n",
    "            concat = True):\n",
    "    # np.random.seed(41)\n",
    "    # Base path\n",
    "    #cwd = os.getcwd()\n",
    "    load_path = base_path + pickle_path\n",
    "    #vocab, embd = save_data_pickle(load_path) #By Manisha - Comment this ones its loaded\n",
    "    \n",
    "    # filename_embeddings = cwd + '/../../glove/glove.6B.50d.txt'\n",
    "\n",
    "    filename_embeddings = base_path + filename_embeddings\n",
    "    \n",
    "    # GloVe embeddings\n",
    "    vocab, embd = loadGloVe(filename_embeddings)\n",
    "    #print(vocab)\n",
    "    #print(embd)\n",
    "    vocab_size = len(vocab)\n",
    "    embedding_dim = len(embd[0])\n",
    "    #print(embd.dtype)\n",
    "    #embedding = np.asarray(embd, dtype = np.float64)\n",
    "    embedding = np.asarray(embd)\n",
    "    #print(vocab)\n",
    "    #print(embd)\n",
    "    #print(embedding)\n",
    "\n",
    "    # Get vocab dict\n",
    "    voc_dict = build_vocDict(vocab)\n",
    "    #print(voc_dict)\n",
    "    # Read and process data\n",
    "    #h, b, y = read_data() # headline / bodies/ labels\n",
    "    \n",
    "    print('Loading Pickle')\n",
    "    #load_path = pickle_path\n",
    "    with open (load_path, 'rb') as fp:\n",
    "        data_dict = pickle.load(fp)\n",
    "    #print(data_dict)\n",
    "    h_ids = data_dict['h_ids']\n",
    "    b_ids = data_dict['b_ids']\n",
    "    y = data_dict['y']\n",
    "    #print(h_ids)\n",
    "    #print(b_ids)\n",
    "    #print(y)\n",
    "    print('finished loading Pickle')\n",
    "    \n",
    "    # Concatenated headline_bodies zero padded np matrices; seq. lengths as np vector\n",
    "    # h_b_ids, h_b_np, seqlen = concatConvert_np(h_ids, b_ids)\n",
    "    # h_np, h_seqlen, b_np, b_seqlen = distinctConvert_np(h_ids, b_ids)\n",
    "\n",
    "    if concat:\n",
    "        h_b_ids, h_b_np, seqlen = concatConvert_np(h_ids, b_ids)\n",
    "        output_dict = {'y':y,\n",
    "                       'h_b_np':h_b_np, \n",
    "                       'seqlen':seqlen}\n",
    "    else:\n",
    "        h_np, h_seqlen, b_np, b_seqlen = distinctConvert_np(h_ids, b_ids)\n",
    "        # Find and delete empty\n",
    "        ind_empty = []\n",
    "        for i in range(np.shape(h_np)[0]):\n",
    "            if ((h_seqlen[i] == 0) or (b_seqlen[i] == 0)):\n",
    "                ind_empty.append(i)\n",
    "        #print('Empty sequences: ', ind_empty)\n",
    "        if (len(ind_empty) > 0):\n",
    "            y = np.delete(y, ind_empty)\n",
    "            h_np = np.delete(h_np, ind_empty, 0)\n",
    "            b_np = np.delete(b_np, ind_empty, 0)\n",
    "            h_seqlen = np.delete(h_seqlen, ind_empty)\n",
    "            b_seqlen = np.delete(b_seqlen, ind_empty)\n",
    "        output_dict = {'y':y,\n",
    "                       'h_np':h_np, \n",
    "                       'b_np':b_np, \n",
    "                       'h_seqlen':h_seqlen,\n",
    "                       'b_seqlen':b_seqlen}\n",
    "    \n",
    "    #Have to check this\n",
    "    config.embed_size = embedding_dim\n",
    "    config.pretrained_embeddings = embedding\n",
    "    config.vocab_size = vocab_size\n",
    "    return config, output_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded GloVe!\n",
      "Reading dataset\n",
      "Total stances: 49972\n",
      "Total bodies: 1683\n",
      "word_list <map object at 0x000002BD0D6A3EF0>\n",
      "Police\n",
      "2332\n",
      "find\n",
      "471\n",
      "mass\n",
      "6866\n",
      "graves\n",
      "31069\n",
      "with\n",
      "59\n",
      "at\n",
      "67\n",
      "least\n",
      "1230\n",
      "15\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'15'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-59-780ec05b30ae>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      6\u001b[0m            \u001b[1;31m# pickle_path = '/glove/twitter50d_h_ids_b_ids_pickle.p',\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m             \u001b[1;31m#concat = False)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 8\u001b[1;33m     \u001b[0mvocab\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0membd\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msave_data_pickle\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mload_path\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-58-d6802270e93a>\u001b[0m in \u001b[0;36msave_data_pickle\u001b[1;34m(outfilename, embedding_type, parserOption)\u001b[0m\n\u001b[0;32m     26\u001b[0m     \u001b[0mh\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mb\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mread_data\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m#read_data(cwd + '/../../') # headline / bodies/ labels\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     27\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 28\u001b[1;33m     \u001b[0mh_ids\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mh_vects\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mwords2ids_vects\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mh\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvoc_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0membd\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mparserOption\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     29\u001b[0m     \u001b[1;31m#Manishacomment\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     30\u001b[0m     '''b_ids, b_vects = words2ids_vects(b, voc_dict, embd, parserOption)\n",
      "\u001b[1;32m<ipython-input-56-8f978866640f>\u001b[0m in \u001b[0;36mwords2ids_vects\u001b[1;34m(sentences, voc_dict, embedding_matrix, option)\u001b[0m\n\u001b[0;32m     72\u001b[0m             \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mword\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     73\u001b[0m             \u001b[1;31m#print(voc_dict[word])\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 74\u001b[1;33m             \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvoc_dict\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mword\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     75\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mword\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mvoc_dict\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;31m# Only add word if in dictionary\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     76\u001b[0m                 \u001b[0mword_index\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mvoc_dict\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mword\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: '15'"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    pickle_path = '/glove/twitter50d_h_ids_b_ids_pickle.p'\n",
    "    load_path = base_path + pickle_path\n",
    "    # config, data_dict = get_data(1028, \n",
    "            #filename_embeddings = '/glove/glove.twitter.27B.50d.txt',\n",
    "           # pickle_path = '/glove/twitter50d_h_ids_b_ids_pickle.p',\n",
    "            #concat = False)\n",
    "    vocab, embd = save_data_pickle(load_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
