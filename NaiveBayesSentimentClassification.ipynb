{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Naive Bayes Classifier for Sentiment Analysis with POS tagging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 559,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.classify.scikitlearn import SklearnClassifier\n",
    "import pandas as pd\n",
    "from nltk.classify import ClassifierI\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer \n",
    "import re\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 560,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_words = []\n",
    "documents = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 561,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Comment</th>\n",
       "      <th>Label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>So there is no way for me to plug it in here i...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Tied to charger for conversations lasting more...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>I have to jiggle the plug to get it to line up...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>If you have several dozen or several hundred c...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Needless to say  I wasted my money.</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             Comment  Label\n",
       "0  So there is no way for me to plug it in here i...      0\n",
       "3  Tied to charger for conversations lasting more...      0\n",
       "5  I have to jiggle the plug to get it to line up...      0\n",
       "6  If you have several dozen or several hundred c...      0\n",
       "8                Needless to say  I wasted my money.      0"
      ]
     },
     "execution_count": 561,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train = pd.read_csv('amazon_cells_labelled.csv')\n",
    "train_positive = train.loc[train['Label']==1]\n",
    "#dataset of all positive reviews\n",
    "train_positive.head()\n",
    "train_negative = train.loc[train['Label']==0]\n",
    "#dataset of all negative reviews\n",
    "train_negative.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Preprocessing the words - cleaning, converting to tokens, removing stop words\n",
    "<br>Then tagging each word with the part of speech tag\n",
    "<br>For sentiment analysis, the adjectives convey the positive or negative sentiment. So we are creating a dictionary of positive adjectives(from positive dataset) and negative adjectives(from negative data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 562,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = list(set(stopwords.words('english')))\n",
    "st = WordNetLemmatizer()\n",
    "freq = pd.Series(' '.join(train['Comment']).split()).value_counts()[:10]\n",
    "\n",
    "for index, row in train_positive.iterrows():\n",
    "    documents.append( (row[\"Comment\"], row[\"Label\"]) )\n",
    "    comment = row[\"Comment\"]\n",
    "    \n",
    "    #cleaning the comment by removing special characters and numbers\n",
    "    cleaned = re.sub(r'[^(a-zA-Z)\\s]',' ', comment)\n",
    "    \n",
    "    #lemmatize the comment\n",
    "    lemmatized = st.lemmatize(cleaned)\n",
    "\n",
    "    #convert the lemmatized comment into tokens\n",
    "    tokenized = word_tokenize(lemmatized)\n",
    "    \n",
    "    #tagging the tokens\n",
    "    pos = nltk.pos_tag(tokenized)\n",
    "\n",
    "    #Removing stop words\n",
    "    stopped = [w for w in pos if not w in stop_words]\n",
    "    \n",
    "    #remove the high frequency words since they dont contribute to the classification \n",
    "    highfreq =  [w for w in stopped if not w in freq]\n",
    "    \n",
    "    #for sentiment classification, forming a list of adjectives from positive reviews\n",
    "    for w in highfreq:\n",
    "         if w[1][0] in ('JJ'):\n",
    "            all_words.append(w[0].lower())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 563,
   "metadata": {},
   "outputs": [],
   "source": [
    "for index, row in train_negative.iterrows():\n",
    "    documents.append( (row[\"Comment\"], row[\"Label\"]) )\n",
    "    comment = row[\"Comment\"]\n",
    "    cleaned = re.sub(r'[^(a-zA-Z)\\s]',' ', comment)\n",
    "    \n",
    "    lemmatized = st.lemmatize(cleaned)\n",
    "    \n",
    "    tokenized = word_tokenize(lemmatized)\n",
    "    \n",
    "    pos = nltk.pos_tag(tokenized)\n",
    "\n",
    "    stopped = [w for w in pos if not w in stop_words]\n",
    "    \n",
    "    #remove the high frequency words since they dont contribute to the classification \n",
    "    highfreq =  [w for w in stopped if not w in freq]\n",
    "\n",
    "    #for sentiment classification, forming a list of adjectives from negative reviews\n",
    "    for w in highfreq:\n",
    "         if w[1][0] in ('JJ'):\n",
    "            all_words.append(w[0].lower())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 564,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['good', 'great', 'razr', 'great', 'impressed', 'original', 'extended', 'good', 'blue', 'good', 'great', 'new', 'best', 'mobile', 'ideal', 'sensitive', 'sure', 'great', 'fine', 'great', 'nice', 'clear', 'great', 'excellent', 'good', 'bulky', 'usable', 'real', 'useful', 'neat', 'pretty', 'sturdy', 'large', 'fine', 'reasonable', 'great', 'happy', 'sound', 'nice', 'great', 'best', 'last', 'several', 'comfortable', 'most', 'great', 'several', 'beautiful', 'great', 'little', 'handy', 'everyday', 'easy', 'great', 'excellent', 'cheaper', 'super', 'sturdy', 'great', 'best', 'tried', 'other', 'best', 'good', 'free', 'good', 'good', 'nice', 'cool', 'black', 'white', 'great', 'good', 'ear', 'comfortable', 'excellent', 'slim', 'light', 'beautiful', 'little', 'i', 'sleek', 'great', 'nice', 'full', 'basic', 'comfortable', 'tried', 'several', 'different', 'first', 'few', 'small', 'accompanied', 'brilliant', 'great', 'good', 'peachy', 'tremendous', 'relative', 'glad', 'funny', 'sketchy', 't', 'great', 'previous', 'cheap', 'fantastic', 'same', 'sharp', 'nice', 'clear', 'great', 'great', 'great', 'good', 'awesome', 'incredible', 'good', 'nice', 'helpful', 'light', 'thin', 'prettier', 'sharp', 'good', 'best', 'available', 'great', 'impressed', 'mega', 'good', 'great', 'good', 'decent', 'able', 'simple', 'great', 'comfortable', 'decent', 'lightweight', 'pleased', 'favorite', 'best', 'authentic', 'nice', 'excited', 'cute', 'worth', 'excellent', 'great', 'comfortable', 'important', 'strong', 'durable', 'best', 'attractive', 'good', 'excellent', 'free', 'good', 'incredible', 'great', 'real', 'good', 'great', 'reversible', 'great', 'good', 'best', 'helpful', 'whole', 'good', 'good', 'impressed', 'small', 'adorable', 'great', 'installed', 'good', 'excellent', 'excellent', 'best', 'great', 'protective', 'good', 'first', 'clever', 'good', 'great', 'great', 'good', 'fast', 'slim', 'new', 'happy', 'able', 'good', 'low', 'comfortable', 'original', 'excellent', 'clear', 'loud', 'clear', 'excellent', 'great', 'great', 'good', 'little', 'organizational', 'whole', 'easier', 'advertised', 'comfortable', 'light', 'clear', 'good', 'awesome', 'long', 'happier', 'excellent', 'good', 'great', 'clear', 'exterior', 'cool', 'many', 'many', 'handsfree', 'best', 'great', 'excellent', 'satisfied', 'better', 'good', 'effective', 'prompt', 'excellent', 'pleased', 'long', 'great', 'awesome', 'several', 'best', 'good', 'fine', 'overall', 'great', 'ill', 'fantastic', 'modest', 'cellular', 'better', 'new', 'best', 'happy', 'big', 'excellent', 'i', 'glad', 'hard', 'wasn', 'high', 'portable', 'great', 'little', 'expensive', 'great', 'great', 'great', 'strong', 'happy', 'easy', 'long', 'comfortable', 'long', 'tremendous', 'worthwhile', 'usefulness', 'sturdy', 't', 'overnight', 'great', 'super', 'small', 'hybrid', 'great', 'bose', 'important', 'excellent', 'fine', 'great', 'adorable', 'clear', 'new', 'good', 'impressed', 'good', 'overall', 'excited', 'sturdy', 'good', 'new', 'great', 'great', 'incrediable', 'razr', 'good', 'good', 'few', 'small', 'sleek', 'impressive', 'practical', 'ample', 'sound', 'glad', 'happier', 'new', 'happy', 'sound', 'good', 'such', 'great', 'total', 'understanding', 'patient', 'cheap', 'little', 'new', 'packaged', 'nice', 'good', 'less', 'happy', 'great', 'same', 'other', 'low', 'most', 'better', 'cool', 'better', 'good', 'great', 'excellent', 'more', 'happy', 'overall', 'new', 'ear', 'comfortable', 'ear', 'comfortible', 'quick', 'original', 'great', 'sure', 'other', 'few', 'long', 'igo', 'different', 'detailed', 'black', 'great', 'great', 'own', 'good', 'much', 'fabulous', 'free', 'excellent', 'last', 'expensive', 'better', 'sturdy', 'nokia', 'pleased', 'easy', 'ear', 'tiny', 'great', 'handsfree', 'fine', 'high', 'happy', 'good', 'low', 'sharp', 'nice', 'more', 'less', 'easy', 'handy', 'detachable', 'fine', 'better', 'real', 'good', 'upbeat', 'clear', 'smallest', 'great', 'comfortable', 'comfortable', 'least', 'entire', 'strong', 'cool', 'most', 'open', 'original', 'best', 'best', 'great', 'good', 'fine', 'sound', 'new', 'helpful', 'functional', 'other', 'nice', 'soft', 'tight', 'good', 't', 'more', 'great', 'easier', 'best', 'expensive', 'good', 'user', 'friendly', 'easy', 'toactivate', 'good', 'great', 'fantastic', 'next', 'free', 'great', 'ear', 'light', 'confortable', 'extended', 'easy', 'low', 'convenient', 'simple', 'much', 'excellent', 'best', 'highest', 'screen', 'good', 'excellent', 'inconspicuous', 'great', 'simple', 'great', 'sure', 'optimal', 'happy', 'good', 'big', 'awful', 'other', 'good', 'great', 'great', 's', 'front', 'excellent', 'good', 'comfortable', 'long', 'other', 'nice', 'many', 'cool', 'decent', 'satisfied', 'better', 'better', 'third', 'numerous', 'quick', 'good', 'good', 'nice', 'excellent', 's', 'other', 's', 'comfortable', 'few', 'good', 'best', 'laptop', 'good', 'good', 'good', 'good', 'best', 'good', 'happy', 'pleased', 'swivel', 'good', 'excellent', 'dual', 'secure', 'good', 'small', 'happier', 'best', 'loud', 'flawless', 'static', 'normal', 'earbud', 'excellent', 'comfortable', 'easy', 'able', 'few', 'exceptional', 'additional', 'official', 't', 's', 'loudest', 'cheaper', 'good', 'wild', 'better', 'hard', 'good', 'great', 'extra', 'happy', 'good', 'fine', 'easy', 'accessable', 'easy', 'clear', 'nice', 'satisifed', 'wow', 'allot', 'ear', 'happy', 'impressed', 'easier', 'high', 'great', 'pretty', 'easy', 'beautiful', 'excellent', 'best', 'fine', 'ear', 'ear', 'more', 'decent', 'several', 'several', 'mere', 'excessive', 'static', 'odd', 'comfortable', 'long', 'last', 'misleading', 't', 'helpful', 'simple', 'little', 'unacceptible', 'unusable', 'more', 'least', 'short', 'worthless', 'right', 'loud', 't', 'disappointed', 'bad', 'different', 'particular', 'other', 'big', 'few', 'quiet', 'other', 'longer', 'good', 'great', 'poor', 'much', 'greater', 'play', 'big', 'disappointed', 'supposedly', 'new', 'huge', 't', 'impressive', 'such', 'high', 'unhappy', 'strong', 'bad', 'such', 'careful', 'earbud', 'dropped', 'several', 'different', 'happy', 'disappointed', 'poor', 'screen', 'black', 'sudden', 'more', 'poor', 'poor', 'better', 'less', 'don', 't', 'enough', 'last', 'long', 'audio', 'poor', 'latest', 'v', 'loud', 'bluetoooth', 'comfortable', 'more', 'voice', 'few', 'first', 'few', 'signal', 'impressed', 't', 'useless', 'strange', 'few', 'few', 'child', 'better', 'horrible', 'disappointed', 'great', 'useless', 'big', 't', 'same', 'big', 'unusable', 'new', 'additional', 'slow', 'strong', 'unbearable', 'poor', 'thin', 'flimsy', 'scary', 't', 'unreliable', 'little', 't', 't', 'terrible', 'many', 'weak', 'old', 'better', 'unreliable', 'uncomfortable', 'poor', 'previous', 'unhappy', 'hear', 'best', 'lasting', 'flip', 'disappointing', 'inexpensive', 'better', 'useless', 'happy', 'terrible', 'full', 'static', 'signal', 'horrible', 'excellent', 'great', 'only', 'more', 'bad', 'fit', 'big', 'worst', 'few', 'good', 'embarassing', 'ear', 'average', 'bad', 'weak', 'worst', 'other', 'sure', 't', 'stupid', 'new', 'new', 'first', 'bad', 'new', 'original', 'last', 'longer', 'dead', 'first', 'clear', 'distorted', 'easy', 'same', 'weird', 'magnetic', 's', 'black', 'low', 't', 'bad', 'displeased', 'built', 'small', 'difficult', 'bad', 'blue', 'good', 'horrible', 'disappointed', 'poor', 'disappointed', 'inexcusable', 'update', 'difficult', 'uncomfortable', 'unknown', 'difficult', 'simple', 'user', 'important', 'pitiful', 'same', 'same', 'max', 'wrong', 'defective', 'able', 'unacceptable', 'quiet', 'earlier', 'hard', 'right', 'impressed', 'only', 'standard', 'little', 'low', 'horrible', 'real', 'easy', 'gentle', 'touch', 'impressed', 'new', 'missed', 'numerous', 'defective', 'most', 'flimsy', 'don', 't', 'low', 'super', 'dont', 'good', 'same', 'flawed', 'terrible', 'fourth', 'bluetooth', 'comfortable', 'last', 'unacceptable', 'due', 'tech', 'few', 'hard', 'difficult', 'recessed', 'difficult', 'difficult', 't', 'terrible', 'real', 'cheap', 'cheap', 't', 'worst', 'crappy', 'worst', 'seeen', 'better', 'old', 'new', 'genuine', 'worth', 'steep', 'same', 'worst', 'extra', 'good', 'real', 'first', 'audio', 'good', 'awful', 'tinny', 'severe', 'other', 'terrible', 't', 'bad', 'extra', 'terrible', 'large', 'heavy', 'promised', 'light', 'tinny', 'first', 'good', 'huge', 'slow', 'interested', 'worst', 'industrial', 'first', 'upload', 'third', 'awful', 'acceptable', 'awful', 'ready', 'prime', 'cool', 'chinese', 'bad', 'biggest', 'ergonomic', 't', 'cool', 'less', 'same', 'same', 'multiple', 'imac', 'external', 'small', 'extra', 'mediocre', 'last', 'good', 'most', 'loud', 'careful', 'disposable', 'few', 'particular', 'wont', 'amp', 'lightweight', 'same', 'right', 'many', 'good', 'first', 'dead', 'cheap', 'old', 'horrible', 'other', 'old', 'much', 'sorry', 'impossible', 'upgrade', 'horrible', 't', 'double', 'same', 'first', 'fine', 'rare', 'real', 'old', 'wasn', 'great', 'useless', 'first', 'good', 'disappointing', 'unreliable', 'good', 'ear', 'black', 'worst', 'horrible', 'horrible', 'bottom', 'uncomfortable', 'few', 'bad', 'only', 'terrible', 'useless', 'current', 'dirty', 'unreliable', 'good', 'important', 'ear', 'terrible', 'last', 'worst', 't', 'ear', 'helpful', 'low', 'cheap', 'high', 'wrong', 'utterly', 'first', 'important', 'terrible', 'ear', 'weak', 'good', 'stupid', 'beep', 'great', 'unintelligible', 't', 'enough', 'next', 'floppy', 'new', 'couple', 'easier', 'worst', 'weak', 'loose', 'screen', 'black', 't', 'low', 'different', 't', 'small', 'tinny', 'wrong', 'certain', 'normal', 'darn', 'lousy', 'hard', 'enough', 'unsatisfactory', 'poor', 't', 'first', 'current', 'much', 'best', 't', 'tricky', 'disappointed', 't', 'lasted', 'disappointed', 'more', 'only', 'red']\n",
      "1016\n"
     ]
    }
   ],
   "source": [
    "print(all_words)\n",
    "print(len(all_words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 565,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['good', 'great', 'razr', 'impressed', 'original', 'extended', 'blue', 'new', 'best', 'mobile', 'ideal', 'sensitive', 'sure', 'fine', 'nice', 'clear', 'excellent', 'bulky', 'usable', 'real', 'useful', 'neat', 'pretty', 'sturdy', 'large', 'reasonable', 'happy', 'sound', 'last', 'several', 'comfortable', 'most', 'beautiful', 'little', 'handy', 'everyday', 'easy', 'cheaper', 'super', 'tried', 'other', 'free', 'cool', 'black', 'white', 'ear', 'slim', 'light', 'i', 'sleek']\n"
     ]
    }
   ],
   "source": [
    "all_words = nltk.FreqDist(all_words)\n",
    "# listing the 50 most frequent words\n",
    "word_features = list(all_words.keys())[:50]\n",
    "print(word_features)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 566,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000\n"
     ]
    }
   ],
   "source": [
    "# function to create a dictionary of features for each comment in the list document.\n",
    "# The keys are the words in word_features \n",
    "# The values of each key are either true or false for whether that feature appears in the review or not\n",
    "def find_features(document):\n",
    "    words = word_tokenize(document)\n",
    "    features = {}\n",
    "    for w in word_features:\n",
    "        features[w] = (w in words)\n",
    "    return features\n",
    "\n",
    "# Creating features for each review\n",
    "featuresets = [(find_features(comment), label) for (comment, label) in documents]\n",
    "print(len(featuresets))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 567,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000\n"
     ]
    }
   ],
   "source": [
    "# Shuffling the documents \n",
    "random.shuffle(featuresets)\n",
    "print(len(featuresets))\n",
    "training_set = featuresets[:500]\n",
    "testing_set = featuresets[500:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 568,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classifier accuracy percent: 63.800000000000004\n",
      "Most Informative Features\n",
      "                   great = True                1 : 0      =     10.0 : 1.0\n",
      "                    best = True                1 : 0      =      5.4 : 1.0\n",
      "                    good = True                1 : 0      =      4.8 : 1.0\n",
      "                    fine = True                1 : 0      =      4.7 : 1.0\n",
      "                    easy = True                1 : 0      =      4.1 : 1.0\n"
     ]
    }
   ],
   "source": [
    "classifier = nltk.NaiveBayesClassifier.train(training_set)\n",
    "\n",
    "print(\"Classifier accuracy percent:\",(nltk.classify.accuracy(classifier, testing_set))*100)\n",
    "\n",
    "classifier.show_most_informative_features(5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
